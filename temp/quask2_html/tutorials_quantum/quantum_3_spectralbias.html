<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Quantum kernels beyond NISQ" href="quantum_4_beyondnisq.html" /><link rel="prev" title="Projected quantum kernels" href="quantum_2_projected.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>Spectral bias in quantum kernels - QuASK 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">QuASK 2.0.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo_nobg.png" alt="Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learn</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_classical/index.html">Intro to classical kernels</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Intro to classical kernels</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_classical/classical_1_linear_to_kernel.html">Linear and Ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_classical/classical_2_kernel_functions.html">Kernel machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_classical/classical_3_svm.html">Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Intro to quantum kernels</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Intro to quantum kernels</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quantum_0_intro.html">Quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantum_1_expressibility.html">Expressibility in quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantum_2_projected.html">Projected quantum kernels</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Spectral bias in quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantum_4_beyondnisq.html">Quantum kernels beyond NISQ</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_quask/index.html">Advanced use of QuASK</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Advanced use of QuASK</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_0_backends.html">Backends in <em>quask</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_1_preprocessing.html">Preprocessing techniques in <em>quask</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_2_evaluators.html">Criteria to evaluate a quantum kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_3_optimizers.html">Algorithms to optimize a quantum kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_4_ensemble.html">Ensemble of kernel machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_5_cli.html">How to use <em>quask</em> without any code</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_applications/index.html">Applications</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Applications</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_applications/applications_1_proton_collision.html">Anomaly detection in proton collision</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Under the hood</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../platform_overview.html">Platform overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Modules</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About quask</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contact_us.html">Contact Us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="spectral-bias-in-quantum-kernels">
<h1>Spectral bias in quantum kernels<a class="headerlink" href="#spectral-bias-in-quantum-kernels" title="Link to this heading">#</a></h1>
<p>Up to this point, we have introduced the concept of a quantum kernel and
discussed its potential applications (<a class="reference external" href="quantum_0_intro.html">Introduction to quantum kernels
0</a>). We have defined what the expressibility of
a quantum kernel is (<a class="reference external" href="quantum_1_expressibility.html">Introduction to quantum kernels
1</a>) and how to limit expressibility
through projected quantum kernels (<a class="reference external" href="quantum_2_projected.html">Introduction to quantum kernels
2</a>).</p>
<p>In this tutorial, we delve deeper into understanding the characteristics
of a quantum kernel and how to determine if and when this tool is
suitable for solving specific tasks. Our investigation is rooted in the
properties of the kernel’s spectrum, which is derived from Mercer’s
theorem and has been further examined in [Can21].</p>
<p>As per Mercer’s theorem, the integral equation:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[T_\kappa[\phi_j](x) = \int \kappa(x, x') \phi_j(x') p(x') dx = \lambda_j \phi_j(x)\]</div>
</div>
<p>decomposes the kernel function <span class="math notranslate nohighlight">\(\kappa\)</span> into an infinite sequence
of orthogonal eigenfunctions <span class="math notranslate nohighlight">\(\{ \phi_j \}_{j=0}^\infty\)</span> and real,
non-negative eigenvalues <span class="math notranslate nohighlight">\(\{ \lambda_j \}_{j=0}^\infty\)</span>. These are
assumed to be ordered in descending order of eigenvalues. The
decomposition can be expressed as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\kappa(x, x') = \sum_{j=0}^\infty \lambda_j \phi_j(x) \phi_j(x').\]</div>
</div>
<p>This decomposition offers valuable insights into the feasibility of
using quantum kernels and their effectiveness for a spcific task. c
task.</p>
<section id="distribution-of-eigenvalues">
<h2>Distribution of eigenvalues<a class="headerlink" href="#distribution-of-eigenvalues" title="Link to this heading">#</a></h2>
<section id="flat-distributions-of-eigenvalues-lead-to-classifiers-with-poor-generalization">
<h3>Flat distributions of eigenvalues lead to classifiers with poor generalization<a class="headerlink" href="#flat-distributions-of-eigenvalues-lead-to-classifiers-with-poor-generalization" title="Link to this heading">#</a></h3>
<p>Consider a kernel function
<span class="math notranslate nohighlight">\(\kappa(x, x') = \langle \zeta(x), \zeta(x') \rangle_\mathcal{H}\)</span>,
with feature map <span class="math notranslate nohighlight">\(\zeta : \mathbb{R}^d \to \mathcal{H}\)</span>.</p>
<p>Assume that <span class="math notranslate nohighlight">\(\kappa(x, x) = 1\)</span>, with <span class="math notranslate nohighlight">\(\lambda_0\)</span> being its
largest eigenvalue, as per Mercer’s decomposition. Let’s assume a
dataset <span class="math notranslate nohighlight">\(\{ (x^j, y^j) \}_{j=1}^m\)</span>, where the labels are defined
as <span class="math notranslate nohighlight">\(y^j = f(x^j)\)</span> with <span class="math notranslate nohighlight">\(f\)</span> as the target function.
Additionally, consider a kernel ridge regressor, denoted as
<span class="math notranslate nohighlight">\(\tilde{f}\)</span>, trained on the given dataset, possibly with
regularization.</p>
<p>For any <span class="math notranslate nohighlight">\(\epsilon \ge 0\)</span>, the following inequality holds:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\lVert f - \tilde{f} \rVert_2 \ge \sqrt{1 - \frac{\lambda_0 m^2}{\epsilon}} \lVert f \rVert_2\]</div>
</div>
<p>with a probability of at least <span class="math notranslate nohighlight">\(1 - \epsilon - \lambda_0 m^4\)</span>.</p>
<p>It’s important to note that the sum of eigenvalues, under our
assumptions, equals 1 [dohmatob]. In the worst-case scenario, with the
smallest possible <span class="math notranslate nohighlight">\(\lambda_0\)</span>, we would have
<span class="math notranslate nohighlight">\(\lambda_0 = \lambda_1 = ... = 1/\text{dim}\mathcal{H}\)</span>. In the
case of infinite-dimensional Hilbert spaces, such as the Gaussian
kernel, <span class="math notranslate nohighlight">\(\lambda\)</span> can even approach 0 as
<span class="math notranslate nohighlight">\(\text{dim}\mathcal{H}=\infty\)</span>. In very large Hilbert spaces, like
those of quantum kernels for a moderately large number of qubits,
<span class="math notranslate nohighlight">\(1/\text{dim}\mathcal{H} = 1/2^n \approx 0\)</span>.</p>
<p>Under these unfavorable conditions, the theorem above implies that for
sufficiently small <span class="math notranslate nohighlight">\(\lambda_0\)</span>, the kernel machine
<span class="math notranslate nohighlight">\(\tilde{f}\)</span> fails to learn any possible function:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\lVert f - \tilde{f} \rVert_2 \ge \lVert f \rVert_2 \text{ as } \lambda_0 \to 0\]</div>
</div>
<p>More detailed insights into these results are provided in [Kub21,
Appendix D].</p>
<p>This theorem underscores that a kernel, whether classical or quantum,
with a flat distribution of eigenvalues, cannot generalize any function.
However, techniques can be employed to restore a favorable distribution
of eigenvalues.</p>
<p>Additionally, in [Hua21], as demonstrated in the previous tutorial: - A
large singular value of the kernel Gram matrix (indicating a low-rank
kernel Gram matrix) possesses favorable properties, and the associated
kernel exhibits a decaying spectrum. - A large singular value of the
kernel Gram matrix (indicating a high-rank kernel Gram matrix) has
unfavorable properties, and the associated kernel possesses a flat
spectrum.</p>
<p>The projected quantum kernel is one of the possible techniques to
address this issue.</p>
</section>
<section id="the-use-of-bandwidths-can-induce-a-non-flat-distribution-of-eigenvalues">
<h3>The use of bandwidths can induce a non-flat distribution of eigenvalues<a class="headerlink" href="#the-use-of-bandwidths-can-induce-a-non-flat-distribution-of-eigenvalues" title="Link to this heading">#</a></h3>
<p>As evident, the expressibility of a kernel has a direct impact on its
spectrum.</p>
<p>For classical kernels <span class="math notranslate nohighlight">\(\kappa\)</span> with infinite-dimensional Hilbert
spaces, such as the Gaussian kernel, the issue is managed by introducing
a variance parameter <span class="math notranslate nohighlight">\(\sigma\)</span>: - With small <span class="math notranslate nohighlight">\(\sigma\)</span> values,
each data point covers a limited region of the RKHS, resulting in very
small inner products between different data points, a flat spectrum for
<span class="math notranslate nohighlight">\(\kappa\)</span>, and high-rank kernel Gram matrices. - Larger
<span class="math notranslate nohighlight">\(\sigma\)</span> values cause each data point to cover a broader region of
the RKHS, leading to larger inner products between different data
points, a decaying spectrum for <span class="math notranslate nohighlight">\(\kappa\)</span>, and low-rank kernel Gram
matrices.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># example with gaussian kernel</span>
</pre></div>
</div>
<p>In the case of quantum kernels, we have observed that projections can
reduce expressibility. However, a simple technique, akin to that used in
Gaussian kernels, can be employed either in conjunction with or as an
alternative to projection to control expressibility. This technique
leverages the fact that any unitary <span class="math notranslate nohighlight">\(U(\cdot)\)</span> essentially
performs rotations, with each parameter serving as a rotational angle.
By constraining these angles within the range
<span class="math notranslate nohighlight">\([0, \beta] \subset [0, 2\pi]\)</span>, we limit the capacity of
<span class="math notranslate nohighlight">\(\kappa\)</span> to disperse vectors across the Hilbert space of the
quantum system. This approach requires a bandwidth parameter
<span class="math notranslate nohighlight">\(\beta\)</span>. [Can22] has formally demonstrated that this technique
restores a favorable spectrum.</p>
<p>In <em>quask</em>, each operation on every ansatz includes a mandatory
bandwidth parameter, ensuring native support for this capability.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># example with quantum kernel and varying bandwidth</span>
</pre></div>
</div>
</section>
</section>
<section id="task-model-alignment">
<h2>Task-model alignment<a class="headerlink" href="#task-model-alignment" title="Link to this heading">#</a></h2>
<p>Mercer’s decomposition of the kernel function provides valuable insights
into its general capabilities. However, this tool also allows us to
quantify how well a specific kernel can adapt to a particular task.</p>
<p>Consider the kernel function <span class="math notranslate nohighlight">\(\kappa\)</span> and its eigendecomposition
<span class="math notranslate nohighlight">\(\{\phi_p\}, \{\lambda_p\}\)</span>. Now, for a specific task <span class="math notranslate nohighlight">\(f\)</span>
and a kernel regression model <span class="math notranslate nohighlight">\(\tilde{f}\)</span> that employs our quantum
kernel, we can express:</p>
<ul class="simple">
<li><p>The machine learning model in the form of
<span class="math notranslate nohighlight">\(\tilde{f}(x) = \sum_{j=0}^\infty \tilde{w}_j \sqrt{\lambda_j} \phi_j(x)\)</span></p></li>
<li><p>The target function can be eigendecomposed using an orthonormal set
of functions, yielding
<span class="math notranslate nohighlight">\(f(x) = \sum_{j=0}^\infty w_j \sqrt{\lambda_j} \phi_j(x)\)</span>.</p></li>
</ul>
<p>Components with small <span class="math notranslate nohighlight">\(\lambda_p\)</span> make a limited contribution to
the kernel function, and consequently, they contribute less to the
kernel machine. This implies that if the target function has a
significant contribution from <span class="math notranslate nohighlight">\(\phi_j\)</span> that is not reflected in
the kernel, that particular component will be challenging to learn.</p>
<p>Based on these observations, we can define a measure of how well a
specific kernel function aligns with a particular task, referred to as
the <em>task model alignment</em>. It is defined as follows:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[C(k) = \frac{\sum_{j = 0}^{k-1} \lambda_j w_j^2}{\sum_{j = 0}^\infty \lambda_j w_j^2}\]</div>
</div>
<p>This metric represents the fraction of <em>power</em> in the top <span class="math notranslate nohighlight">\(k\)</span>
components of the target function. If the target function concentrates
most of its power in the initial kernel components, then the kernel
machine will generalize effectively.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># todo evaluate task model alignment</span>
</pre></div>
</div>
</section>
<section id="exponential-concentration-of-kernel-values">
<h2>Exponential concentration of kernel values<a class="headerlink" href="#exponential-concentration-of-kernel-values" title="Link to this heading">#</a></h2>
<p>A final, challenging aspect of poorly designed kernels affected by an
excess of expressibility must be examined. In both classical and quantum
kernels, we have observed that one consequence of expressibility is that
most inner products tend to diminish as the dimension of the Hilbert
space increases.</p>
<p>In classical kernels, we can observe this with random projection, as
follows:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># todo</span>
</pre></div>
</div>
<p>Similarly, for Gaussian kernels with small <span class="math notranslate nohighlight">\(\sigma\)</span>, we have:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># todo</span>
</pre></div>
</div>
<p>This behavior also extends to quantum kernels. Notably, when
<span class="math notranslate nohighlight">\(\lambda_0 = \lambda_1 = ... = O(2^{-n})\)</span>, [Tha22] has
demonstrated that <span class="math notranslate nohighlight">\(\kappa(x, x') \in O(2^{-n})\)</span> for almost all
<span class="math notranslate nohighlight">\(x, x'\)</span>. In the realm of quantum computing, where the kernel
values are estimated rather than precisely calculated, this has
significant implications for the algorithm’s scalability.</p>
<p>Specifically, if we require an accuracy of <span class="math notranslate nohighlight">\(\epsilon = 1/2^n\)</span> to
distinguish zero and nonzero kernel values, we would need
<span class="math notranslate nohighlight">\(O(1/\epsilon^2) = O(2^{2n})\)</span> shots to estimate the value
correctly. This phenomenon is referred to as the <em>exponential
concentration of kernel values</em>.</p>
</section>
<section id="references-and-acknowledgments">
<h2>References and acknowledgments<a class="headerlink" href="#references-and-acknowledgments" title="Link to this heading">#</a></h2>
<p>[Can21] Canatar, A., Bordelon, B., &amp; Pehlevan, C. (2021). Spectral bias
and task-model alignment explain generalization in kernel regression and
infinitely wide neural networks. Nature communications, 12(1), 2914.</p>
<p>[Can22] Canatar, A., Peters, E., Pehlevan, C., Wild, S. M., &amp; Shaydulin,
R. (2022). Bandwidth enables generalization in quantum kernel models.
arXiv preprint arXiv:2206.06686.</p>
<p>[Kub21] Kübler, J., Buchholz, S., &amp; Schölkopf, B. (2021). The inductive
bias of quantum kernels. Advances in Neural Information Processing
Systems, 34, 12661-12673.</p>
<p>[dohmatob]
<a class="reference external" href="https://mathoverflow.net/questions/391248/analytic-formula-for-the-eigenvalues-of-kernel-integral-operator-induced-by-lapl">https://mathoverflow.net/questions/391248/analytic-formula-for-the-eigenvalues-of-kernel-integral-operator-induced-by-lapl</a></p>
<p>[Hua21] Huang, HY., Broughton, M., Mohseni, M. et al. Power of data in
quantum machine learning. Nat Commun 12, 2631 (2021).
<a class="reference external" href="https://doi.org/10.1038/s41467-021-22539-9">https://doi.org/10.1038/s41467-021-22539-9</a></p>
<p>[Tha22] Thanasilp, S., Wang, S., Cerezo, M., &amp; Holmes, Z. (2022).
Exponential concentration and untrainability in quantum kernel methods.
arXiv preprint arXiv:2208.11060.</p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="quantum_4_beyondnisq.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Quantum kernels beyond NISQ</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="quantum_2_projected.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Projected quantum kernels</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Massimiliano Incudini, Michele Grossi
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Spectral bias in quantum kernels</a><ul>
<li><a class="reference internal" href="#distribution-of-eigenvalues">Distribution of eigenvalues</a><ul>
<li><a class="reference internal" href="#flat-distributions-of-eigenvalues-lead-to-classifiers-with-poor-generalization">Flat distributions of eigenvalues lead to classifiers with poor generalization</a></li>
<li><a class="reference internal" href="#the-use-of-bandwidths-can-induce-a-non-flat-distribution-of-eigenvalues">The use of bandwidths can induce a non-flat distribution of eigenvalues</a></li>
</ul>
</li>
<li><a class="reference internal" href="#task-model-alignment">Task-model alignment</a></li>
<li><a class="reference internal" href="#exponential-concentration-of-kernel-values">Exponential concentration of kernel values</a></li>
<li><a class="reference internal" href="#references-and-acknowledgments">References and acknowledgments</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=51b770b3"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>