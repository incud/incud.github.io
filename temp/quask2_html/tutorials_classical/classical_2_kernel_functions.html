<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Support Vector Machines" href="classical_3_svm.html" /><link rel="prev" title="Linear and Ridge regression" href="classical_1_linear_to_kernel.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>Kernel machines - QuASK 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">QuASK 2.0.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo_nobg.png" alt="Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learn</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Intro to classical kernels</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Intro to classical kernels</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="classical_1_linear_to_kernel.html">Linear and Ridge regression</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Kernel machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="classical_3_svm.html">Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_quantum/index.html">Intro to quantum kernels</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Intro to quantum kernels</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_0_intro.html">Quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_1_expressibility.html">Expressibility in quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_2_projected.html">Projected quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_3_spectralbias.html">Spectral bias in quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_4_beyondnisq.html">Quantum kernels beyond NISQ</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_quask/index.html">Advanced use of QuASK</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Advanced use of QuASK</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_0_backends.html">Backends in <em>quask</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_1_preprocessing.html">Preprocessing techniques in <em>quask</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_2_evaluators.html">Criteria to evaluate a quantum kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_3_optimizers.html">Algorithms to optimize a quantum kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_4_ensemble.html">Ensemble of kernel machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_5_cli.html">How to use <em>quask</em> without any code</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_applications/index.html">Applications</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Applications</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_applications/applications_1_proton_collision.html">Anomaly detection in proton collision</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Under the hood</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../platform_overview.html">Platform overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Modules</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About quask</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contact_us.html">Contact Us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="kernel-machines">
<h1>Kernel machines<a class="headerlink" href="#kernel-machines" title="Link to this heading">#</a></h1>
<p>In the <a class="reference external" href="classical_1_linear_to_kernel.html">previous tutorial</a>, we
introduced linear models for regression. These models are effective
under certain assumptions, primarily that the target function to be
learned is linear. In this tutorial, we’ll explore how to adapt Ridge
regression models to capture more complex relationships within the data
while maintaining efficient trainin.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The [‘Data Mining Book’](<a class="reference external" href="https://dataminingbook.info/book_html/">https://dataminingbook.info/book_html/</a>) provides excellent material that can be consulted online for free.</p>
</div>
<section id="feature-map-and-kernel-functions">
<h2>Feature map and kernel functions<a class="headerlink" href="#feature-map-and-kernel-functions" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> belong to the input space
<span class="math notranslate nohighlight">\(\mathcal{X} = \mathbb{R}^d\)</span>. A <em>feature map</em>, denoted as
<span class="math notranslate nohighlight">\(\phi: \mathbb{R}^d \to \mathcal{H}\)</span>, is a transformation of input
attributes into a Hilbert space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. This mapping allows
us to represent the original features in a richer way, allowing us to
solve the learning problem more effectively.</p>
<p>Consider a scenario where our target function is given by
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = v_0 \mathbf{x}_1^2 + v_1 \mathbf{x}_1 \mathbf{x}_2 + v_2 \mathbf{x}^2\)</span>.
This target function exhibits quadratic relationships within the
features, making it unsuitable for learning using a simple linear
regressor in the form
<span class="math notranslate nohighlight">\(\tilde{f}(\mathbf{x}) = w_1 \mathbf{x}_1 + w_2 \mathbf{x}_2\)</span>. To
address this issue, we can define a feature map:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\phi(\mathbf{x}) = \left[\begin{array}{c} \mathbf{x}_1 &amp; \mathbf{x}_2 &amp; \mathbf{x}_1 \mathbf{x}_2 &amp; \mathbf{x}_1^2 &amp;  \mathbf{x}_2^2 \end{array}\right]^\top\]</div>
</div>
<p>and then use a linear regressor that operates directly on the
transformed vector,
<span class="math notranslate nohighlight">\(\tilde{f} = \sum_{j=1}^5 w_j \phi(\mathbf{x})_j\)</span>. In situations
where the dual form of the linear (Ridge) regressor is used, we can
directly replace the Euclidean inner product
<span class="math notranslate nohighlight">\(\langle \cdot, \cdot \rangle\)</span> with the <em>kernel function</em>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\kappa(\mathbf{x}, \mathbf{x}') = \langle \phi(\mathbf{x}), \phi(\mathbf{x}') \rangle_\mathcal{H}\]</div>
</div>
<section id="calculating-the-kernel-can-be-more-convenient-than-calculating-the-explicit-representation-of-the-feature-map">
<h3>Calculating the kernel can be more convenient than calculating the explicit representation of the feature map<a class="headerlink" href="#calculating-the-kernel-can-be-more-convenient-than-calculating-the-explicit-representation-of-the-feature-map" title="Link to this heading">#</a></h3>
<p>A major advantage of using a kernel function is the efficiency it offers
in calculating complex relationships without explicitly representing
<span class="math notranslate nohighlight">\(\phi(\mathbf{x})\)</span>. This is particularly evident in cases like the
polynomial kernel, designed to capture polynomial relationships of
arbitrary degrees within the data:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\kappa(\mathbf{x}, \mathbf{x}') = (\langle \mathbf{x}, \mathbf{x}' \rangle + b)^c\]</div>
</div>
<p>Its feature map has <span class="math notranslate nohighlight">\(k = \sum_{j = 1}^c \binom{c}{j}\)</span> components,
<span class="math notranslate nohighlight">\(\phi : \mathbb{R}^d \to \mathbb{R}^k\)</span>, with <span class="math notranslate nohighlight">\(k \gg d\)</span>. In
many cases, the Hilbert space of the feature map has a much higher
dimensionality than the original input space.</p>
<p>A more striking example is the Gaussian or RBF kernel:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\kappa(\mathbf{x}, \mathbf{x}') = \exp(-c \lVert \mathbf{x} - \mathbf{x}' \rVert_2^2)\]</div>
</div>
<p>Its calculation is straightforward, but the underlying feature map
transforms <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> into a Gaussian function
with a mean value in <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> itself,
<span class="math notranslate nohighlight">\(\phi(\mathbf{x}) \in L_2(\mathbb{R}^d)\)</span>. In this case,
<span class="math notranslate nohighlight">\(\mathcal{H} = L_2(\mathbb{R}^d)\)</span> is infinite-dimensional, and
constructing an explicit representation is unfeasible. A naive attempt
would require value discretization and truncation to finite intervals.</p>
</section>
<section id="positive-semidefiniteness-of-kernel-functions">
<h3>Positive semidefiniteness of kernel functions<a class="headerlink" href="#positive-semidefiniteness-of-kernel-functions" title="Link to this heading">#</a></h3>
<p>The most important fact about a kernel function is that it represents an
inner product in some Hilbert space. To determine if a bilinear form is
a valid kernel, we only need to ensure that it behaves like an inner
product, which means it has to be positive semidefinite. This latter
property also implies that the function is symmetric.</p>
<p>If we can prove that <span class="math notranslate nohighlight">\(\kappa\)</span> is a positive semidefinite bilinear
form, then there exists a (non-unique) Hilbert space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>
and a feature map <span class="math notranslate nohighlight">\(\phi\)</span> satisfying
<span class="math notranslate nohighlight">\(\kappa(x, x') = \langle \phi(x), \phi(x') \rangle_\mathcal{H}\)</span>.
This is true even if we don’t know their exact definitions or how to
compute them explicitly. Furthermore, positive semidefiniteness implies
that, given data <span class="math notranslate nohighlight">\(\mathbf{x}^1, ..., \mathbf{x}^m\)</span>, the kernel
Gram matrix <span class="math notranslate nohighlight">\(K_{i,j} = \kappa(\mathbf{x}^i, \mathbf{x}^j)\)</span> is
positive semidefinite.</p>
<p>Conversely, it is trivially true that if we define a kernel explicitly
from the feature map, positive semidefiniteness holds by construction.</p>
</section>
<section id="reproducing-kernel-hilbert-space">
<h3>Reproducing Kernel Hilbert Space<a class="headerlink" href="#reproducing-kernel-hilbert-space" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\kappa\)</span> be a kernel function. We have infinitely many feature
maps and Hilbert spaces corresponding to the given kernel. However,
there is a unique <em>reproducing kernel feature map</em>
<span class="math notranslate nohighlight">\(\Phi_x : \mathcal{X} \to \mathbb{R}\)</span>, defined as
<span class="math notranslate nohighlight">\(\Phi_x = \kappa(\cdot, \mathbf{x})\)</span>. We can use such a feature
map to define the following pre-Hilbert vector space:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{V} = \mathrm{span}\{ \Phi_x \mid x \in \mathcal{X} \} = \left\{f(\cdot) = \sum_{i=1}^n \alpha_i \kappa(\cdot, x^i) \mid n \in \mathbb{N}, x^i \in \mathcal{X} \right\}.\]</div>
</div>
<p>We can prove that the following function is an inner product on
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, which means it is symmetric, bilinear, and positive
semidefinite:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[ \langle f, g \rangle
= \left\langle \sum_{i} \alpha_i \kappa(\cdot, x^i),  \sum_{j} \beta_j \kappa(\cdot, x^j) \right\rangle
= \sum_{i, j} \alpha_i \beta_j \kappa(x^i, x^j).\]</div>
</div>
<p>We can also prove that <span class="math notranslate nohighlight">\(\kappa\)</span> has the reproducing property,
which means that the following equation holds:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\langle f, \kappa(\cdot, x^j)\rangle = f(x^j).\]</div>
</div>
<p>We can define the vector space
<span class="math notranslate nohighlight">\(\mathcal{H} = \overline{\mathcal{V}}\)</span>, which is complete and,
thus, a Hilbert space. This latter one is denoted as the Reproducing
Kernel Hilbert Space (RKHS) of <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
</section>
</section>
<section id="kernel-ridge-regression">
<h2>Kernel Ridge regression<a class="headerlink" href="#kernel-ridge-regression" title="Link to this heading">#</a></h2>
<p>We have defined the Ridge regressor in the previous tutorial,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{f}(\mathbf{x}) = \sum_{j=1}^m \alpha_j \langle \mathbf{x}, \mathbf{x}^j \rangle\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\({\alpha} = (G + \lambda I)^{-1} \mathbf{y}\)</span> is the solution
of the optimization problem expressed via the Lagrangian multipliers,
and <span class="math notranslate nohighlight">\(G = X^\top X\)</span> Gram matrix. To define a <em>kernel</em> Ridge
regressor we just have to substitute the inner product with the kernel
function, and the Gram matrix <span class="math notranslate nohighlight">\(G\)</span> with the <em>kernel</em> Gram matrix
<span class="math notranslate nohighlight">\(K_{i,j} = \kappa(x^i, x^j)\)</span>,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{f}(\mathbf{x}) = \sum_{j=1}^m \alpha_j \kappa(\mathbf{x}, \mathbf{x}^j).\]</div>
</div>
<p>We can easily test the model on a synthetic dataset:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">m</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">unknown_f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">3.2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">5.2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">unknown_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.33</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,))</span>
</pre></div>
</div>
<p>To test the approach, we need to define a kernel. Given the problem’s
structure, we choose a polynomial kernel of degree three. As this is a
Ridge regression, the parameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> must be set, which corresponds
to the strength of the regularization term.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="kn">import</span> <span class="n">KernelRidge</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>

<span class="n">kernel_regressor</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;polynomial&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">kernel_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">kernel_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error: </span><span class="si">{</span><span class="n">mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Mean</span> <span class="n">Squared</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.03978691374195227</span>
</pre></div>
</div>
<p>The kernel has been specified, in this example, in the arguments
<code class="docutils literal notranslate"><span class="pre">kernel</span></code> and <code class="docutils literal notranslate"><span class="pre">degree</span></code>. Look at the documentation to see all the
possibilities offere built-in in the <em>scikit-learn</em> package.</p>
<section id="underfitting">
<h3>Underfitting<a class="headerlink" href="#underfitting" title="Link to this heading">#</a></h3>
<p>When we select a kernel that is not sophisticated enough to capture the
inherent relationships within the dataset, we end up with a model that
cannot effectively learn the target function. This phenomenon is known
as <em>underfitting</em>, and it occurs when both the training set and testing
set errors are high.</p>
<p>In our example, this may occur if we use a linear kernel when the actual
function is cubic:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="kn">import</span> <span class="n">KernelRidge</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>

<span class="n">kernel_regressor</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;polynomial&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">kernel_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">kernel_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">kernel_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error training: </span><span class="si">{</span><span class="n">mse_train</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error testing: </span><span class="si">{</span><span class="n">mse_test</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Mean</span> <span class="n">Squared</span> <span class="n">Error</span> <span class="n">training</span><span class="p">:</span> <span class="mf">0.3511775695752495</span>
<span class="n">Mean</span> <span class="n">Squared</span> <span class="n">Error</span> <span class="n">testing</span><span class="p">:</span> <span class="mf">0.5276843667598432</span>
</pre></div>
</div>
</section>
<section id="overfitting">
<h3>Overfitting<a class="headerlink" href="#overfitting" title="Link to this heading">#</a></h3>
<p>When we select a kernel that is too sophisticated, the model
interpolates both the data and the noise within the dataset. This
results in a model whose underlying function is extremely complicated
and distant from the true target. This phenomenon is known as
<em>overfitting</em>, and it occurs when the training set error is low, and the
testing set error is high.</p>
<p>In this case, setting a large regularization constant can mitigate the
problem. A large regularization constant favors ‘simple’ solutions over
complicated ones, even if they better interpolate the data.</p>
<p>In our example, this may occur if we use a degree-50 kernel when the
actual function is ubic:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># disable warning about singular matrices</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">LinAlgWarning</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">LinAlgWarning</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s1">&#39;sklearn&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="kn">import</span> <span class="n">KernelRidge</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">kernel_regressor</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;polynomial&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">kernel_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">kernel_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">kernel_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error training: </span><span class="si">{</span><span class="n">mse_train</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error testing: </span><span class="si">{</span><span class="n">mse_test</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Mean</span> <span class="n">Squared</span> <span class="n">Error</span> <span class="n">training</span><span class="p">:</span> <span class="mf">3.117245929034068e-05</span>
<span class="n">Mean</span> <span class="n">Squared</span> <span class="n">Error</span> <span class="n">testing</span><span class="p">:</span> <span class="mf">24185905.35320369</span>
</pre></div>
</div>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="classical_3_svm.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Support Vector Machines</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="classical_1_linear_to_kernel.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Linear and Ridge regression</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Massimiliano Incudini, Michele Grossi
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Kernel machines</a><ul>
<li><a class="reference internal" href="#feature-map-and-kernel-functions">Feature map and kernel functions</a><ul>
<li><a class="reference internal" href="#calculating-the-kernel-can-be-more-convenient-than-calculating-the-explicit-representation-of-the-feature-map">Calculating the kernel can be more convenient than calculating the explicit representation of the feature map</a></li>
<li><a class="reference internal" href="#positive-semidefiniteness-of-kernel-functions">Positive semidefiniteness of kernel functions</a></li>
<li><a class="reference internal" href="#reproducing-kernel-hilbert-space">Reproducing Kernel Hilbert Space</a></li>
</ul>
</li>
<li><a class="reference internal" href="#kernel-ridge-regression">Kernel Ridge regression</a><ul>
<li><a class="reference internal" href="#underfitting">Underfitting</a></li>
<li><a class="reference internal" href="#overfitting">Overfitting</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=51b770b3"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>