<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Intro to quantum kernels" href="../tutorials_quantum/index.html" /><link rel="prev" title="Kernel machines" href="classical_2_kernel_functions.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>Support Vector Machines - QuASK 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">QuASK 2.0.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo_nobg.png" alt="Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learn</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Intro to classical kernels</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Intro to classical kernels</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="classical_1_linear_to_kernel.html">Linear and Ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="classical_2_kernel_functions.html">Kernel machines</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_quantum/index.html">Intro to quantum kernels</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Intro to quantum kernels</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_0_intro.html">Quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_1_expressibility.html">Expressibility in quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_2_projected.html">Projected quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_3_spectralbias.html">Spectral bias in quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_4_beyondnisq.html">Quantum kernels beyond NISQ</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_quask/index.html">Advanced use of QuASK</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Advanced use of QuASK</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_0_backends.html">Backends in <em>quask</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_1_preprocessing.html">Preprocessing techniques in <em>quask</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_2_evaluators.html">Criteria to evaluate a quantum kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_3_optimizers.html">Algorithms to optimize a quantum kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_4_ensemble.html">Ensemble of kernel machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_5_cli.html">How to use <em>quask</em> without any code</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_applications/index.html">Applications</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Applications</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_applications/applications_1_proton_collision.html">Anomaly detection in proton collision</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Under the hood</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../platform_overview.html">Platform overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Modules</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About quask</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contact_us.html">Contact Us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="support-vector-machines">
<h1>Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Link to this heading">#</a></h1>
<p>In the previous tutorials, we have demonstrated how to solve a
regression problem using a linear regressor and established that the
training of such a model is efficient. We also pointed out that linear
regressors work best when the assumption is made that the target
function is linear. However, when such an assumption does not hold, we
can employ kernel methods to enable the linear regressor to capture
nonlinear relationships within the data.</p>
<p>In this tutorial, we will explore how to address a binary classification
problem using a model known as the Support Vector Machine. This model is
designed to find the optimal hyperplane that separates two classes. The
classification problem can be solved when the dataset is linearly
separable, meaning there exists a hyperplane that perfectly separates
the two classes. However, we will also examine the use of a soft margin
to accommodate slight errors in the dataset labels and the application
of kernel methods to enable non-linear classifiers.</p>
<p>Additionally, we will delve into using the same model for solving an
unsupervised learning problem known as anomaly detetion.</p>
<section id="classifying-linearly-separable-data-the-hard-margin-svm">
<h2>Classifying linearly-separable data: the <em>hard-margin SVM</em><a class="headerlink" href="#classifying-linearly-separable-data-the-hard-margin-svm" title="Link to this heading">#</a></h2>
<p>Linear regression is the simplest machine learning model used for
learning a target function, denoted as <span class="math notranslate nohighlight">\(f\)</span>. It requires a dataset
of samples, <span class="math notranslate nohighlight">\(\{ (\mathbf{x}^j, y^j) \}_{j=1}^m\)</span>. Each feature
vector is represented as <span class="math notranslate nohighlight">\(\mathbf{x}^j \in \mathbb{R}^d\)</span>, and has
been sampled i.i.d. from some unknown probability distribution
<span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span>. The labels are either <span class="math notranslate nohighlight">\(-1\)</span> or <span class="math notranslate nohighlight">\(1\)</span>,
defined as <span class="math notranslate nohighlight">\(y^j\)</span>.</p>
<p>Suppose that the dataset is linearly separable. Then, there exists a
hyperplane <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x} + b = 0\)</span>, where
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is the normal vector to the hyperplane and <span class="math notranslate nohighlight">\(b\)</span>
the offset from the origin. In this case, we can define two parallel
hyperplanes such that all the elements of one class, the ones having
<span class="math notranslate nohighlight">\(y^j=1\)</span>, stay in the region
<span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}^j + b \ge 1\)</span> and all the others, the
ones having <span class="math notranslate nohighlight">\(y^j=-1\)</span>, stay in the region
<span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}^j + b \le -1\)</span>. These two conditions
can be summarized in one single inequality,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[y^j(\mathbf{w}^\top \mathbf{x}^j + b) \ge 1.\]</div>
</div>
<p>Among all the possible pairs of parallel hyperplanes, we want to choose
the best one, which is the one that maximizes the margin between the two
classes. Since the distance between the two hyperplanes is
<span class="math notranslate nohighlight">\(2/\lVert \mathbf{w} \rVert\)</span>, the problem can be formulated as</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\arg\min_{w,b} &amp; \langle w, w \rangle \\
\text{constrained to} &amp; y^j(\mathbf{w}^\top \mathbf{x}^j + b) \ge 1, \text{with }j = 1, ..., m
\end{array}\end{split}\]</div>
</div>
<p>Once found the weight vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, the estimator takes the
form</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{f}(\mathbf{x}) = \mathrm{sign}(\mathbf{w}^\top \mathbf{x}^j + b).\]</div>
</div>
<section id="solving-the-svm-with-the-lagrangian-multipliers">
<h3>Solving the SVM with the Lagrangian multipliers<a class="headerlink" href="#solving-the-svm-with-the-lagrangian-multipliers" title="Link to this heading">#</a></h3>
<p>We can solve the learning problem using the method of the Lagrangian
multipliers,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{w}, \mathbf{\alpha}) = \frac{1}{2} \langle w, w \rangle - \sum_{j=1}^m \mathbf{\alpha}_j [y^j (\mathbf{x}^j \mathbf{w} + b) - 1].\]</div>
</div>
<p>We can solve the primal form of this Lagrangian,
<span class="math notranslate nohighlight">\(\arg\min_{w, b} \max_{\alpha} \mathcal{L}(\mathbf{w}, \mathbf{\alpha})\)</span>,
using a quadratic optimization solve, as the objective is quadratic and
the constraint on each point forms a convex set.</p>
<p>We can solve the dual form of this Lagrangian,
<span class="math notranslate nohighlight">\(\arg \max_{\alpha} \min_{w, b} \mathcal{L}(\mathbf{w}, \mathbf{\alpha})\)</span>.
First, calculate</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = \mathbf{w} - \sum_{j=1}^m \mathbf{\alpha}_j y^j \mathbf{x}^j = 0 &amp; \implies \mathbf{w} = \sum_{j=1}^m \mathbf{\alpha}_j y^j \mathbf{x}^j. \\
\frac{\partial \mathcal{L}}{\partial b} = - \sum_{j=1}^m \mathbf{\alpha}_j y^j = 0 &amp; \implies \sum_{j=1}^m \mathbf{\alpha}_j y^j = 0.
\end{array}\end{split}\]</div>
</div>
<p>Then,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{l}
\frac{1}{2} \langle \sum_{j=1}^m \mathbf{\alpha}_j y^j \mathbf{x}^j, \sum_{k=1}^m \mathbf{\alpha}_k y^k \mathbf{x}^k \rangle - \sum_{j=1}^m \mathbf{\alpha}_j [y^j (\mathbf{x}^j (\sum_{k=1}^m \mathbf{\alpha}_k y^k \mathbf{x}^k) + b) - 1] \\
= \frac{1}{2} \sum_{j,k=1}^m \mathbf{\alpha}_j \mathbf{\alpha}_k y^j y^k \langle \mathbf{x}^j, \mathbf{x}^k \rangle
- \frac{1}{2} \sum_{j,k=1}^m \mathbf{\alpha}_j \mathbf{\alpha}_k y^j y^k \langle \mathbf{x}^j, \mathbf{x}^k \rangle
- \cancel{\sum_{j=1}^m \mathbf{\alpha}_j y^j b}
+ \sum_{j=1}^m \mathbf{\alpha}_j \\
= - \frac{1}{2} \sum_{j,k=1}^m \mathbf{\alpha}_j \mathbf{\alpha}_k y^j y^k \langle \mathbf{x}^j, \mathbf{x}^k \rangle
+ \sum_{j=1}^m \mathbf{\alpha}_j
\end{array}.\end{split}\]</div>
</div>
<p>Again, the solution of <span class="math notranslate nohighlight">\(\alpha\)</span> can be found using a quadratic
optimization solver. You can see how to solve this, from an
implementation perspective, following <a class="reference external" href="https://github.com/Girrajjangid/Machine-Learning-from-Scratch/blob/master/Support%20Vector%20Machine/2.%20SVM%20with%20hard%20margin%20(from%20scratch).ipynb">this series of
tutorial</a>.</p>
</section>
<section id="sparsity-of-the-solution">
<h3>Sparsity of the Solution<a class="headerlink" href="#sparsity-of-the-solution" title="Link to this heading">#</a></h3>
<p>The points lying on the hyperplanes are referred to as <em>support
vectors</em>, and they correspond to nonzero coefficients <span class="math notranslate nohighlight">\(\alpha_j\)</span>.
These support vectors are the only points that contribute to the
prediction. Due to the optimization problem we have defined, which
prefers <span class="math notranslate nohighlight">\(\alpha_j = 0\)</span> as the optimal situation, the solution
tends to be sparse. In other words, SVMs typically have only a few
support vectors. This characteristic enhances the efficiency of
predictions, particularly when compared to linear regression.</p>
</section>
</section>
<section id="allowing-few-errors-the-soft-margin-svm">
<h2>Allowing few errors: the <em>soft-margin SVM</em><a class="headerlink" href="#allowing-few-errors-the-soft-margin-svm" title="Link to this heading">#</a></h2>
<p>In situations where the data is not linearly separable or is noisy,
finding a single hyperplane that perfectly separates the two classes may
not be possible. In such cases, we can employ a classification technique
that allows for a certain degree of error. This variant of SVM is known
as the “soft-margin SVM.”</p>
<p>The objective function is reformulated to include a set of auxiliary
variables, often referred to as slack variables:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\xi_j = \max(0, 1 -  y^j(\mathbf{w}^\top \mathbf{x}^j + b))\]</div>
</div>
<p>Here, for <span class="math notranslate nohighlight">\(\xi_j = 0\)</span>, it indicates that the <span class="math notranslate nohighlight">\(j\)</span>-th point
has been correctly classified and lies beyond its class’s margin. If
<span class="math notranslate nohighlight">\(0 &lt; \xi_j &lt; 1\)</span>, it means the point is correctly classified but
falls between its margin and the middle hyperplane separating the two
margins. When <span class="math notranslate nohighlight">\(\xi_j \ge 1\)</span>, it implies that the point is
misclassified.</p>
<section id="c-svm">
<h3><span class="math notranslate nohighlight">\(C\)</span>-SVM<a class="headerlink" href="#c-svm" title="Link to this heading">#</a></h3>
<p>The problem can be reformulated as follows:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\arg\min_{w, b, \xi \ge 0} &amp; \langle w, w \rangle + C \sum_{j=1}^m \xi_j \\
\text{subject to} &amp; y^j(\mathbf{w}^\top \mathbf{x}^j + b) \ge 1 - \xi_j \text{ for } j = 1, ..., m
\end{array}\end{split}\]</div>
</div>
<p>Here, <span class="math notranslate nohighlight">\(C\)</span> is a parameter that trades off between classification
accuracy and margin size. For <span class="math notranslate nohighlight">\(C = \infty\)</span>, every error, no matter
how small, results in a solution with an infinite cost, effectively
recovering the previously mentioned “hard-margin” SVM. This formulation
can still be solved using quadratic optimization techniques. This form
is called C-SVM.</p>
<p>The dual form is identical to the hard-margin SVM, except for the
additional constraint <span class="math notranslate nohighlight">\(0 \le \alpha_j \le C\)</span>.</p>
</section>
<section id="nu-svm">
<h3><span class="math notranslate nohighlight">\(\nu\)</span>-SVM<a class="headerlink" href="#nu-svm" title="Link to this heading">#</a></h3>
<p>The <span class="math notranslate nohighlight">\(\nu\)</span>-SVM (nu-SVM) substitutes the parameter <span class="math notranslate nohighlight">\(C\)</span> with
<span class="math notranslate nohighlight">\(\nu \in [0,1)\)</span>, which acts as an upper bound on the <em>fraction</em> of
margin errors and a lower bound on the fraction of support vectors.</p>
<p>The formulation in its primal form is:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\arg\min_{w, b, \xi \ge 0, \rho&gt;0} &amp; \frac{1}{2} \langle w, w \rangle - \nu\rho + \frac{1}{2} \sum_{j=1}^m \xi_j \\
\text{subject to} &amp; y^j(\mathbf{w}^\top \mathbf{x}^j + b) \ge \rho - \xi_j \text{ for } j = 1, ..., m
\end{array}\end{split}\]</div>
</div>
<p>Note that <span class="math notranslate nohighlight">\(\rho\)</span> substitute <span class="math notranslate nohighlight">\(1\)</span> in the constraint, meaning
the margins are not distant <span class="math notranslate nohighlight">\(2/\lVert w \rVert\)</span> anymore, but they
are distant <span class="math notranslate nohighlight">\(2\rho/\lVert w \rVert\)</span>. The dual form is shown in
[chen05].</p>
<p>If the solution of the optimization problem leads to <span class="math notranslate nohighlight">\(\rho &gt; 0\)</span>,
then the <span class="math notranslate nohighlight">\(\nu\)</span>-SVM is equivalent to a <span class="math notranslate nohighlight">\(C\)</span>-SVM with
<span class="math notranslate nohighlight">\(C = \frac{1}{m\rho}\)</span>.</p>
</section>
</section>
<section id="allowing-non-linear-boundaries-the-kernel-svm">
<h2>Allowing non-linear boundaries: the <em>kernel SVM</em><a class="headerlink" href="#allowing-non-linear-boundaries-the-kernel-svm" title="Link to this heading">#</a></h2>
<p>Any dual formulation of the above described <span class="math notranslate nohighlight">\(C\)</span>- and
<span class="math notranslate nohighlight">\(\nu\)</span>-SVM can be used with kernel methods immediately.</p>
<p>We can create an example of a classification problem using the <em>iris</em>
dataset in <em>scikit-learn</em>, which aims to classify the three different
species of Iris by looking at the size of their petals and sepals.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">NuSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Load the Iris dataset, pick only the first two classes</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">,:</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># restrict to the first two classes (first 100 elements) and only 2 features up to 4</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>

<span class="c1"># Split the dataset into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create and train the Nu-SVC model. We choose a two-degree polynomial kernel</span>
<span class="n">nu_svm</span> <span class="o">=</span> <span class="n">NuSVC</span><span class="p">(</span><span class="n">nu</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">nu_svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">nu_svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate the accuracy of the model</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Accuracy</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
</pre></div>
</div>
<p>It is possible also to visualize the decision boundary found by the SVM,
as it follows:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Create a mesh grid to plot the decision boundary</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">))</span>

<span class="c1"># Get predictions for each point in the mesh grid</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">nu_svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Plot the decision boundary</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Plot the data points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="c1"># Highlight support vectors</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">nu_svm</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">nu_svm</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="c1"># Set plot labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Nu-SVM Decision Boundary&#39;</span><span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/output_11_0.png" src="../_images/output_11_0.png" />
</section>
<section id="novelty-and-outlier-detection">
<h2>Novelty and outlier detection<a class="headerlink" href="#novelty-and-outlier-detection" title="Link to this heading">#</a></h2>
<p>Intro…</p>
<p>This kind of problem can be <em>unsupervised</em>, meaning the training usually
do not have (or do not care about) the labels. In fact, the primary form
of the optimization problem is</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\arg\min_{w, b, \xi \ge 0, \rho&gt;0} &amp; \frac{1}{2} \langle w, w \rangle - \rho + \frac{1}{\nu d} \sum_{j=1}^m \xi_j \\
\text{subject to} &amp; (\mathbf{w}^\top \mathbf{x}^j + b) \ge \rho - \xi_j \text{ for } j = 1, ..., m
\end{array}\end{split}\]</div>
</div>
<p>and all the <span class="math notranslate nohighlight">\(y^j\)</span> are disappeared from the constraints.
Furthermore, <span class="math notranslate nohighlight">\(d\)</span> is the dimensionality of the feature vectors.</p>
<p>In the literature, novelty detection and outlier detection as used as
synonyms. Sometimes, novelty detection denotes a training process for
which we train uniquely on the regular data, while outlier detection is
trained on both regular and anomalous data.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">OneClassSVM</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Select the first two classes, separately</span>
<span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">y_1</span><span class="p">,</span> <span class="n">y_2</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span>

<span class="c1"># Create the training set, use only one class</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_1</span><span class="p">[:</span><span class="mi">25</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">row_stack</span><span class="p">([</span><span class="n">X_1</span><span class="p">[</span><span class="mi">25</span><span class="p">:</span><span class="mi">50</span><span class="p">],</span> <span class="n">X_2</span><span class="p">[:</span><span class="mi">50</span><span class="p">]])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">25</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Create a One-Class SVM model for outlier detection</span>
<span class="n">one_class_svm</span> <span class="o">=</span> <span class="n">OneClassSVM</span><span class="p">(</span><span class="n">nu</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">one_class_svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># 1 = normal data, -1 = anomalies</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">one_class_svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate the accuracy of the model</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">75</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">75</span><span class="p">,)</span>
<span class="n">Accuracy</span><span class="p">:</span> <span class="mf">96.00</span><span class="o">%</span>
</pre></div>
</div>
<p>We can also visualize the result as before. Only one point has been
misclassified.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a mesh grid to plot the decision boundary</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">))</span>

<span class="c1"># Get predictions for each point in the mesh grid</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">one_class_svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Plot the decision boundary with blue and orange colors</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Plot the data points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="c1"># Set plot labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;One-Class SVM Outlier Detection&#39;</span><span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/output_15_0.png" src="../_images/output_15_0.png" />
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p>[chen05] Chen and Lin and Schölkopf. (2005). A tutorial on nu‐support
vector machines. Applied Stochastic Models in Business and Industry,
21(2), 111-136.</p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../tutorials_quantum/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Intro to quantum kernels</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="classical_2_kernel_functions.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Kernel machines</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Massimiliano Incudini, Michele Grossi
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Support Vector Machines</a><ul>
<li><a class="reference internal" href="#classifying-linearly-separable-data-the-hard-margin-svm">Classifying linearly-separable data: the <em>hard-margin SVM</em></a><ul>
<li><a class="reference internal" href="#solving-the-svm-with-the-lagrangian-multipliers">Solving the SVM with the Lagrangian multipliers</a></li>
<li><a class="reference internal" href="#sparsity-of-the-solution">Sparsity of the Solution</a></li>
</ul>
</li>
<li><a class="reference internal" href="#allowing-few-errors-the-soft-margin-svm">Allowing few errors: the <em>soft-margin SVM</em></a><ul>
<li><a class="reference internal" href="#c-svm"><span class="math notranslate nohighlight">\(C\)</span>-SVM</a></li>
<li><a class="reference internal" href="#nu-svm"><span class="math notranslate nohighlight">\(\nu\)</span>-SVM</a></li>
</ul>
</li>
<li><a class="reference internal" href="#allowing-non-linear-boundaries-the-kernel-svm">Allowing non-linear boundaries: the <em>kernel SVM</em></a></li>
<li><a class="reference internal" href="#novelty-and-outlier-detection">Novelty and outlier detection</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=51b770b3"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>