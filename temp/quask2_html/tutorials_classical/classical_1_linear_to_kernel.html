<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Kernel machines" href="classical_2_kernel_functions.html" /><link rel="prev" title="Intro to classical kernels" href="index.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>Linear and Ridge regression - QuASK 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">QuASK 2.0.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo_nobg.png" alt="Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learn</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Intro to classical kernels</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Intro to classical kernels</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Linear and Ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="classical_2_kernel_functions.html">Kernel machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="classical_3_svm.html">Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_quantum/index.html">Intro to quantum kernels</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Intro to quantum kernels</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_0_intro.html">Quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_1_expressibility.html">Expressibility in quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_2_projected.html">Projected quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_3_spectralbias.html">Spectral bias in quantum kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quantum/quantum_4_beyondnisq.html">Quantum kernels beyond NISQ</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_quask/index.html">Advanced use of QuASK</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Advanced use of QuASK</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_0_backends.html">Backends in <em>quask</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_1_preprocessing.html">Preprocessing techniques in <em>quask</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_2_evaluators.html">Criteria to evaluate a quantum kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_3_optimizers.html">Algorithms to optimize a quantum kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_4_ensemble.html">Ensemble of kernel machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_quask/quask_5_cli.html">How to use <em>quask</em> without any code</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_applications/index.html">Applications</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Applications</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials_applications/applications_1_proton_collision.html">Anomaly detection in proton collision</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Under the hood</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../platform_overview.html">Platform overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Modules</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About quask</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contact_us.html">Contact Us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="linear-and-ridge-regression">
<h1>Linear and Ridge regression<a class="headerlink" href="#linear-and-ridge-regression" title="Link to this heading">#</a></h1>
<p>Our first tutorial delves into the motivation behind the application of
kernel methods. We begin by demonstrating regression analysis using the
simplest model, the linear regressor. Next, we explore how Ridge
regularization can be applied to mitigate numerical errors during model
training. Finally, we delve into solving these learning problems using
the Lagrangian multiplier methods, highlighting how this method gives
rise to the primal and dual forms of the Ridge regressor.</p>
<section id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h2>
<p>Linear regression is the simplest machine learning model used for
learning a target function, denoted as <span class="math notranslate nohighlight">\(f\)</span>. It requires a dataset
of samples, <span class="math notranslate nohighlight">\(\{ (\mathbf{x}^j, y^j) \}_{j=1}^m\)</span>. Each feature
vector is represented as <span class="math notranslate nohighlight">\(\mathbf{x}^j \in \mathbb{R}^d\)</span>, and has
been sampled i.i.d. from some unknown probability distribution
<span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span>. The labels are real numbers, defined as
<span class="math notranslate nohighlight">\(y^j = f(\mathbf{x}^j) + \varepsilon^i\)</span>, with each
<span class="math notranslate nohighlight">\(\varepsilon^i\)</span> representing random Gaussian noise with zero mean
and fixed variance.</p>
<p>Let
<span class="math notranslate nohighlight">\(X = \left[ \begin{array}{c} (\mathbf{x}^1)^\top \\ \vdots \\ (\mathbf{x}^m)^\top \end{array}\right] \in \mathbb{R}^{m \times d}\)</span>,
which is the design matrix, and
<span class="math notranslate nohighlight">\(\mathbf{y} = \left[ \begin{array}{c} y^1 \\ \vdots \\ y^m \end{array}\right] \in \mathbb{R}^{m \times 1}\)</span>,
representing the regressand.</p>
<p>We can generate randomly a synthetic dataset for our demo:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># set the dimensionality of the problem: number of samples of the dataset, dimensionality of the feature vector</span>
<span class="n">m</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>

<span class="c1"># create a target function f to learn, according to our assumptions it should be a linear function</span>
<span class="n">unknown_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,))</span>
<span class="n">unknown_f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">unknown_w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># generate the synthetic dataset: first the features...</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

<span class="c1"># generate the synthetic dataset: ... and then the noisy labels</span>
<span class="n">noiseless_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">unknown_f</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">noiseless_y</span> <span class="o">+</span> <span class="n">noise</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset dimensionality    : design matrix </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">=}</span><span class="s2"> | regressand </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Example of feature vector : </span><span class="si">{</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Example of label          : </span><span class="si">{</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Dataset</span> <span class="n">dimensionality</span>    <span class="p">:</span> <span class="n">design</span> <span class="n">matrix</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">|</span> <span class="n">regressand</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,)</span>
<span class="n">Example</span> <span class="n">of</span> <span class="n">feature</span> <span class="n">vector</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.57980451</span> <span class="mf">0.18366092</span><span class="p">]</span>
<span class="n">Example</span> <span class="n">of</span> <span class="n">label</span>          <span class="p">:</span> <span class="mf">0.18893031545613453</span>
</pre></div>
</div>
<p>We are going to build a function:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{f}(\mathbf{x}) = \langle \mathbf{x}, \mathbf{w}\rangle + b\]</div>
</div>
<p>such that <span class="math notranslate nohighlight">\(\tilde{f}\)</span> is as close as possible to <span class="math notranslate nohighlight">\(f\)</span>. Note
that we will omit the bias <span class="math notranslate nohighlight">\(b\)</span> [foot1]. The ideal scenario would
be to find the <span class="math notranslate nohighlight">\(\tilde{f}\)</span>, or the vector of weights
<span class="math notranslate nohighlight">\(\tilde{\mathbf{w}}\)</span>, that minimizes the <em>expected risk</em>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{f}(\mathbf{x}) = \arg\min_h \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x})}\left[(h(\mathbf{x}) - f(y))^2\right]\]</div>
</div>
<p>However, we have no access to <span class="math notranslate nohighlight">\(f\)</span> other than the given dataset, so
we can only minimize the <em>empirical risk</em>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{f}(\mathbf{x}) = \arg\min_h \sum_{j=1}^m (h(\mathbf{x}^j) - y^j)^2\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{w} = \arg\min_{w \in \mathbb{R}^d} (\langle \mathbf{x}^j, \mathbf{w}\rangle - y^j)^2\]</div>
</div>
<p>Provided that the columns of <span class="math notranslate nohighlight">\(X\)</span> are linearly independent, the
problem has a unique, analytical solution obtained by setting the
derivative of the objective function to zero,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\nabla_w \left[\lVert X w - y \rVert\right] &amp; = 0 \\
\nabla_w \left[(Xw)^\top Xw-(Xw)^\top y - y^\top(Xw)+y^\top y\right] &amp; = 0 \\
\nabla_w \left[w^\top X^\top X w-2(Xw)^\top y + y^\top y\right] &amp; = 0 \\
2 X^\top X w - 2 X^\top y &amp; = 0 \\
X^\top X w &amp; = X^\top y
\end{array}\end{split}\]</div>
</div>
<p>which results in the following solution,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{w} = (X^\top X)^{-1} X^\top \mathbf{y}.\]</div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># standard matrix inversion, not numerically stable</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># more stable way to calculate matrix inversion</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># finish calculating</span>
<span class="n">estimated_w</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># the empirical risk is not zero because of the noise in the dataset label</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Empirical risk between target and estimated functions: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">:</span><span class="w"> </span><span class="n">estimated_w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># nonetheless, we get decent generalization error</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True risk between target and estimated functions: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">estimated_w</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">unknown_w</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Empirical</span> <span class="n">error</span> <span class="n">between</span> <span class="n">target</span> <span class="ow">and</span> <span class="n">estimated</span> <span class="n">functions</span><span class="p">:</span> <span class="mf">0.35801910246331653</span>
<span class="kc">True</span> <span class="n">error</span> <span class="n">between</span> <span class="n">target</span> <span class="ow">and</span> <span class="n">estimated</span> <span class="n">functions</span><span class="p">:</span> <span class="mf">0.06560906515727047</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using linear regression, we are making certain assumptions, such as assuming that the target function is linear, the data has been independently and identically distributed (i.i.d.) from some distribution, and the noise’s variance is constant. If the target function does <em>not</em> adhere to these assumptions, it will be challenging to solve the learning task with a linear classifier, and we should consider choosing a different, more suitable machine learning model.</p>
</div>
<p>Using the capabilities of the <em>scikit-learn</em> framework instead of
writing everything from scratch with NumPy is much easier and less
error-prone. The same example demonstrated earlier can be rephrased as
follows:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># create model</span>
<span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># training</span>
<span class="n">lin_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># retrieve the weight parameters</span>
<span class="n">another_estimated_w</span> <span class="o">=</span> <span class="n">lin_reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weights with the LinearRegressor class:&quot;</span><span class="p">,</span> <span class="n">another_estimated_w</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Weights</span> <span class="k">with</span> <span class="n">the</span> <span class="n">LinearRegressor</span> <span class="n">class</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.19503256</span> <span class="mf">0.14576726</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="ridge-regression">
<h2>Ridge regression<a class="headerlink" href="#ridge-regression" title="Link to this heading">#</a></h2>
<p>In solving the learning problem above, numerical errors can arise when
computing the inverse of <span class="math notranslate nohighlight">\(X^\top X\)</span>, especially when it has nearly
singular values. To address this issue, we can introduce positive
elements on the principal diagonal. This adjustment reduces the
condition number and eases matrix inversion. In this case, we aim to
solve the following problem:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{w} = \arg\min_{w \in \mathbf{R}^d} \sum_{j=1}^m (\langle \mathbf{x}^j, \mathbf{w}\rangle - y^j)^2 + \lambda \lVert \mathbf{w} \rVert_2\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\lambda \ge 0\)</span> is the regularization term.</p>
<p>A regressor whose associated learning problem is the one described above
is called a <em>ridge regressor</em>. The solution can still be found
analytically:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\tilde{w} = (X^\top X + \lambda I)^{-1} X^\top \mathbf{y}.\]</div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">rigde_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">rigde_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ridge_estimated_w</span> <span class="o">=</span> <span class="n">rigde_reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weights with the Ridge class:&quot;</span><span class="p">,</span> <span class="n">ridge_estimated_w</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Weights</span> <span class="k">with</span> <span class="n">the</span> <span class="n">Ridge</span> <span class="n">class</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.15521267</span> <span class="mf">0.13250445</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="the-lagrangian-multiplier-method">
<h2>The Lagrangian multiplier method<a class="headerlink" href="#the-lagrangian-multiplier-method" title="Link to this heading">#</a></h2>
<p>The solution to the <em>ridge regularization</em> problem can be found via the
Lagrangian multipliers methods. It is applied with problems in the
following form:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl} \min_w &amp; f(w) \\ \text{constrained to} &amp; h_j(w) = 0, \text{with }j = 1, ..., k\end{array}\end{split}\]</div>
</div>
<p>We assume that both <span class="math notranslate nohighlight">\(f\)</span> and any <span class="math notranslate nohighlight">\(h_j\)</span> is convex. The
Lagrangian function is defined as</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{w}, \mathbf{\alpha}) = f(\mathbf{w}) + \sum_{j=1}^k \mathbf{\alpha}_j h_j(\mathbf{w})\]</div>
</div>
<p>and the parameters <span class="math notranslate nohighlight">\(\mathbf{\alpha}_j\)</span> are called Lagrangian
multipliers. The solution of this optimization problem can be found by
setting the partial derivatives to zero,
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = 0, \frac{\partial \mathcal{L}}{\partial \mathbf{\alpha}} = 0\)</span>
and solve for <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\alpha}\)</span>.</p>
<p>In this case, we can solve two possible optimization problems,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\min_w \max_\alpha \mathcal{L}(w, \alpha)\]</div>
</div>
<p>which is the <em>primal</em> form of the optimization problem, or</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\max_\alpha \min_w \mathcal{L}(w, \alpha)\]</div>
</div>
<p>which is the <em>dual</em> form. In general the solution of the two opimization
problems are different, but, under some assumptions (the KKT conditions)
they coincide. These assutptions hold for the optimization problem
underlying the Rigde regression.</p>
<section id="solving-the-ridge-regression-with-lagrangian-multipliers">
<h3>Solving the Ridge regression with Lagrangian multipliers<a class="headerlink" href="#solving-the-ridge-regression-with-lagrangian-multipliers" title="Link to this heading">#</a></h3>
<p>When considering the following objective function,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{w}) = \frac{1}{2}\sum_{j=1}^m (\mathbf{w}^\top \mathbf{x}^j - y^j)^2 + \frac{\lambda}{2} \langle \mathbf{w}, \mathbf{w}\rangle,\]</div>
</div>
<p>the optimization problem <span class="math notranslate nohighlight">\(\arg\min_w \mathcal{L}(w)\)</span> is
unconstrained, and it is solved analytically without the Lagrangian
multiplier method. The <span class="math notranslate nohighlight">\(1/2\)</span> factor simplifies the constant when
taking the derivative and does not influence the result of the
optimization.</p>
<p>We can, although, convert this problem into a constraint optimization.
It seems that we are making things more difficult, but truly this way
will reveal important insights. We introduce the variables
<span class="math notranslate nohighlight">\(\mathbf{z}_j = \mathbf{w}^\top \mathbf{x}^j - y^j\)</span>, and this
variable can be associated with the constraint
<span class="math notranslate nohighlight">\(h_j(\mathbf{z}, \mathbf{w}) \equiv \mathbf{z}_j - \mathbf{w}^\top \mathbf{x}^j + y^j = 0\)</span>.
The optimization problem becomes</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\arg\min_w &amp; \frac{1}{2} \langle z, z \rangle + \frac{\lambda}{2} \langle w, w \rangle \\
\text{constrained to} &amp; h_j(w) = 0, \text{with }j = 1, ..., m
\end{array}\end{split}\]</div>
</div>
<p>Now, a trick to make the result look nicer is to multiply the objective
function by <span class="math notranslate nohighlight">\(1/\lambda\)</span>, while the constraints remain untouched.
Again, this does not change the overall result.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\arg\min_w &amp; \frac{1}{2\lambda} \langle z, z \rangle + \frac{1}{2} \langle w, w \rangle \\
\text{constrained to} &amp; h_j(w) = 0, \text{with }j = 1, ..., m
\end{array}\end{split}\]</div>
</div>
<p>For each constraint, we define a Lagrangian multiplier <span class="math notranslate nohighlight">\(\alpha_j\)</span>.
The associated Lagrangian is:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{w}, \mathbf{z}, \mathbf{\alpha})
= \frac{1}{2\lambda} \langle \mathbf{z}, \mathbf{z} \rangle
+ \frac{1}{2} \langle \mathbf{w}, \mathbf{w}\rangle
+ \mathbf{\alpha}^\top (\mathbf{z} - X \mathbf{w} + \mathbf{y}).\]</div>
</div>
<p>Solving this problem in its primal form leads to the solution that we
have already found. However, if we solve the dual form,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\max_\alpha \min_{w, z} \mathcal{L}(\mathbf{w}, \mathbf{z}, \mathbf{\alpha}),\]</div>
</div>
<p>we obtain that</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = \mathbf{w} - \mathbf{\alpha}^\top X = 0 &amp; \implies \mathbf{w}^\top = X^\top \mathbf{\alpha}. \\
\frac{\partial \mathcal{L}}{\partial \mathbf{z}} = \frac{1}{\lambda} \mathbf{z} + \mathbf{\alpha} = 0 &amp; \implies \mathbf{z} =  - \lambda \mathbf{\alpha}.
\end{array}\end{split}\]</div>
</div>
<p>Once substituted <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(z\)</span>, the remaining objective takes
the form:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl}
\mathcal{L}(\mathbf{\alpha})
&amp; = \frac{1}{2\lambda} \langle -\lambda\mathbf{\alpha}, -\lambda\mathbf{\alpha} \rangle
+ \frac{1}{2} \langle X^\top \mathbf{\alpha}, X^\top \mathbf{\alpha}\rangle
+ \mathbf{\alpha}^\top (-\lambda\mathbf{\alpha} - X X^\top \mathbf{\alpha} + \mathbf{y}) \\
&amp; = \frac{\lambda}{2} \mathbf{\alpha}^\top \mathbf{\alpha}
+ \frac{1}{2} \mathbf{\alpha}^\top X X^\top \mathbf{\alpha}
- \lambda \mathbf{\alpha}^\top \mathbf{\alpha}
- \mathbf{\alpha}^\top X X^\top \mathbf{\alpha}
+ \mathbf{\alpha}^\top \mathbf{y} \\
&amp; = - \frac{\lambda}{2} \mathbf{\alpha}^\top \mathbf{\alpha}
- \frac{1}{2} \mathbf{\alpha}^\top X X^\top \mathbf{\alpha}
+ \mathbf{\alpha}^\top \mathbf{y} \\
&amp; = - \frac{1}{2} \mathbf{\alpha}^\top (\lambda I) \mathbf{\alpha}
- \frac{1}{2} \mathbf{\alpha}^\top X X^\top \mathbf{\alpha}
+ \mathbf{\alpha}^\top \mathbf{y} \\
&amp; = - \frac{1}{2} \mathbf{\alpha}^\top (X X^\top + \lambda I) \mathbf{\alpha}
+ \mathbf{\alpha}^\top \mathbf{y} \\
&amp; = - \frac{1}{2} \mathbf{\alpha}^\top (G + \lambda I) \mathbf{\alpha}
+ \mathbf{\alpha}^\top \mathbf{y}
\end{array}\end{split}\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(G = X X^\top\)</span> is the Gram matrix of the inner products
between <span class="math notranslate nohighlight">\(x^1, ..., x^m\)</span>. The solution is</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\nabla_\alpha \mathcal{L} = - (G + \lambda I) \alpha + \mathbf{y} = 0 \implies \tilde\alpha = (G + \lambda I)^{-1} \mathbf{y}.\]</div>
</div>
<p>Now, we have two ways of expressing the same Ridge regressor: * In the
primal form, we have
<span class="math notranslate nohighlight">\(\tilde{f}(\mathbf{x}) = \sum_{j=1}^d \mathbf{w}_j \mathbf{x}^j\)</span>.
* In the dual form, we have
<span class="math notranslate nohighlight">\(\tilde{f}(\mathbf{x}) = \sum_{j=1}^m \alpha_j \langle \mathbf{x}, \mathbf{x}^j \rangle\)</span>.</p>
<p>The primal form has the advantage that, in case we are dealing with a
large amount of data (<span class="math notranslate nohighlight">\(m \gg d\)</span>), the regressor is much more
efficient. The dual form has the advantage of expressing the regressor
in terms of its similarity to the elements of the training set, instead
of on <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> itself. This leads to new possibilities:
changing the notion of ‘similarity’ allows the creation of much more
powerful models.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p>[foot1] as it can be incorporated as the weight vector
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span> via the equation
<span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x} + b = [\mathbf{w}, b]^\top [\mathbf{x}, 1] = (\mathbf{w}')^\top \mathbf{x}'\)</span>.</p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="classical_2_kernel_functions.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Kernel machines</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="index.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Intro to classical kernels</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Massimiliano Incudini, Michele Grossi
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Linear and Ridge regression</a><ul>
<li><a class="reference internal" href="#linear-regression">Linear Regression</a></li>
<li><a class="reference internal" href="#ridge-regression">Ridge regression</a></li>
<li><a class="reference internal" href="#the-lagrangian-multiplier-method">The Lagrangian multiplier method</a><ul>
<li><a class="reference internal" href="#solving-the-ridge-regression-with-lagrangian-multipliers">Solving the Ridge regression with Lagrangian multipliers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=51b770b3"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>